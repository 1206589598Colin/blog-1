---
title: 分库分表基本原理
categories: ['分布式']
tags: ['分布式', '分片']
date: 2019-10-16 20:54
---

# 分库分表基本原理

> 分片（Sharding）的基本思想就要把一个数据库切分成多个部分，存储在不同的数据库(server)上，从而缓解单一数据库的性能问题。
>
> 分库分表一定是为了支撑 **高并发、数据量大** 两个问题的。
>
> 📦 本文已归档到：「[blog](https://github.com/dunwu/blog)」

## 一、为何要分库分表

分库分表主要基于以下理由：

- **并发连接** - 单库超过每秒 2000 个并发时，而一个健康的单库最好保持在每秒 1000 个并发左右，不要太大。
- **磁盘容量** - 磁盘容量占满，会导致服务器不可用。
- **SQL 性能** - 单表数据量过大，会导致 SQL 执行效率低下。一般，单表有 200 万条数据，就可以考虑分表了。

| #            | 分库分表前                   | 分库分表后                                   |
| ------------ | ---------------------------- | -------------------------------------------- |
| 并发支撑情况 | MySQL 单机部署，扛不住高并发 | MySQL 从单机到多机，能承受的并发增加了多倍   |
| 磁盘使用情况 | MySQL 单机磁盘容量几乎撑满   | 拆分为多个库，数据库服务器磁盘使用率大大降低 |
| SQL 执行性能 | 单表数据量太大，SQL 越跑越慢 | 单表数据量减少，SQL 执行效率明显提升         |

## 二、分库分表原理

数据分片指按照某个维度将存放在单一数据库中的数据分散地存放至多个数据库或表中以达到提升性能瓶颈以及可用性的效果。 数据分片的有效手段是对关系型数据库进行分库和分表。分库和分表均可以有效的避免由数据量超过可承受阈值而产生的查询瓶颈。 除此之外，分库还能够用于有效的分散对数据库单点的访问量；分表虽然无法缓解数据库压力，但却能够提供尽量将分布式事务转化为本地事务的可能，一旦涉及到跨库的更新操作，分布式事务往往会使问题变得复杂。 使用多主多从的分片方式，可以有效的避免数据单点，从而提升数据架构的可用性。

通过分库和分表进行数据的拆分来使得各个表的数据量保持在阈值以下，以及对流量进行疏导应对高访问量，是应对高并发和海量数据系统的有效手段。 数据分片的拆分方式又分为垂直分片和水平分片。

### 读写分离

主服务器用来处理写操作以及实时性要求比较高的读操作，而从服务器用来处理读操作。

读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。

读写分离的优点在于：

- 有效减少锁竞争 - 主服务器只负责写，从服务器只负责读，能够有效的避免由数据更新导致的行锁竞争，使得整个系统的查询性能得到极大的改善。
- 提高查询吞吐量 - 通过一主多从的配置方式，可以将查询请求均匀的分散到多个数据副本，能够进一步的提升系统的处理能力。
- 提升数据库可用性 - 使用多主多从的方式，不但能够提升系统的吞吐量，还能够提升数据库的可用性，可以达到在任何一个数据库宕机，甚至磁盘物理损坏的情况下仍然不影响系统的正常运行。

![img](http://dunwu.test.upcdn.net/cs/database/mysql/master-slave-proxy.png)

读写分离则是根据 SQL 语义的分析，将读操作和写操作分别路由至主库与从库。

![读写分离](https://shardingsphere.apache.org/document/current/img/read-write-split/read-write-split.png)

读写分离的数据节点中的数据内容是一致的，而水平分片的每个数据节点的数据内容却并不相同。将水平分片和读写分离联合使用，能够更加有效的提升系统性能。

读写分离虽然可以提升系统的吞吐量和可用性，但同时也带来了数据不一致的问题。 这包括多个主库之间的数据一致性，以及主库与从库之间的数据一致性的问题。 并且，读写分离也带来了与数据分片同样的问题，它同样会使得应用开发和运维人员对数据库的操作和运维变得更加复杂。 下图展现了将分库分表与读写分离一同使用时，应用程序与数据库集群之间的复杂拓扑关系。

![数据分片 + 读写分离](https://shardingsphere.apache.org/document/current/img/read-write-split/sharding-read-write-split.png)

### 垂直分片

垂直分片有两种拆分考量：业务拆分和访问频率拆分

（1）业务拆分

> 业务拆分的核心理念是**专库专用**。

在拆分之前，一个数据库由多个数据表构成，每个表对应着不同的业务。而拆分之后，则是**按照业务将表进行归类，分布到不同的数据库中**，从而将压力分散至不同的数据库。下图展示了根据业务需要，将用户表和订单表垂直分片到不同的数据库的方案。

垂直分片往往需要对架构和设计进行调整。通常来讲，是来不及应对互联网业务需求快速变化的；而且，它也并无法真正的解决单点瓶颈。**垂直拆分可以缓解数据量和访问量带来的问题，但无法根治。如果垂直拆分之后，表中的数据量依然超过单节点所能承载的阈值，则需要水平分片来进一步处理**。

（2）访问频率拆分

> 访问频率拆分，是 **把一个有很多字段的表给拆分成多个表，或者是多个库上去**。一般来说，会 **将较少的、访问频率较高的字段放到一个表中**，然后 **将较多的、访问频率较低的字段放到另外一个表中**。因为数据库是有缓存的，访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。

![image-20200114211639899](http://dunwu.test.upcdn.net/snap/image-20200114211639899.png)

一般来说，满足下面的条件就可以考虑扩容了：

- Mysql 单库超过 5000 万条记录，Oracle 单库超过 1 亿条记录，DB 压力就很大。
- 单库超过每秒 2000 个并发时，而一个健康的单库最好保持在每秒 1000 个并发左右，不要太大。

在数据库的层面使用垂直切分将按数据库中表的密集程度部署到不同的库中，例如将原来的电商数据库垂直切分成商品数据库、用户数据库等。

### 水平分片

> **水平拆分** 又称为 **Sharding**，它是将同一个表中的记录拆分到多个结构相同的表中。当 **单表数据量太大** 时，会极大影响 **SQL 执行的性能** 。分表是将原来一张表的数据分布到数据库集群的不同节点上，从而缓解单点的压力。

相对于垂直分片，水平分片不再将数据根据业务逻辑分类，而是通过某个字段（或某几个字段），根据某种规则将数据分散至多个库或表中，每个分片仅包含数据的一部分。 例如：根据主键分片，偶数主键的记录放入 0 库（或表），奇数主键的记录放入 1 库（或表）。

水平分片从理论上突破了单机数据量处理的瓶颈，并且扩展相对自由，是分库分表的标准解决方案。

![image-20200114211203589](http://dunwu.test.upcdn.net/snap/image-20200114211203589.png)

一般来说，**单表有 200 万条数据** 的时候，性能就会相对差一些了，需要考虑分表了。但是，这也要视具体情况而定，可能是 100 万条，也可能是 500 万条，SQL 越复杂，就最好让单表行数越少。

### 分库分表策略

分库分表策略主要有两种：

- 根据数值范围划分
- 根据 Hash 划分

#### 数值范围划分

数值范围划分，就是根据 ID、时间范围 这类具有排序性的字段来进行划分。例如：用户 Id 为 1-9999 的记录分到第一个库，10000-20000 的分到第二个库，以此类推。

按这种策略划分出来的数据，具有数据连续性。

优点：数据迁移很简单。

缺点：容易产生热点问题，大量的流量都打在最新的数据上了。

#### Hash 划分

典型的 Hash 划分，如根据数值取模，当需要扩容时，一般以 2 的幂次方进行扩容（这样，扩容时迁移的数据量会小一些）。例如：用户 Id mod n，余数为 0 的记录放到第一个库，余数为 1 的放到第二个库，以此类推。

优点：数据离散分布，不存在热点问题。

缺点：数据迁移、扩容麻烦（之前的数据需要重新计算 hash 值重新分配到不同的库或表）。

## 三、迁移和扩容

### 停机迁移/扩容（不推荐）

停机迁移/扩容是最暴力、最简单的迁移、扩容方案。

![](http://dunwu.test.upcdn.net/snap/20200601114836.png)

#### 停机迁移/扩容流程

（0）预估停服时间，发布停服公告。

（1）停服，不允许数据访问。

（2）编写临时的数据导入程序，从老数据库中读取数据。

（3）将数据写入中间件。

（4）中间件根据分片规则，将数据分发到分库（分表）中。

（5）应用程序修改配置，重启。

#### 停机迁移/扩容方案分析

优点：简单、没有数据一致性问题。

缺点：如果老的数据库数据量很大，则停机处理时间可能很久。比如老的数据库是已经分库分表的数据库群，数据量可能达到亿级，导入数据可能就要花费几小时。如果中间过程中出现问题，就容易引发重大事故。

点评：综上，这种方案代价太高，不可取。

### 双写迁移

![](http://dunwu.test.upcdn.net/snap/20200601135751.png)

#### 双写迁移流程

（1）修改应用程序配置，将数据同时写入老数据库和中间件。这就是所谓的**双写**，同时写俩库，老库和新库。

（2）编写临时程序，读取老数据库。

（3）将数据写入中间件。如果数据不存在，直接写入；如果数据存在，比较时间戳，只允许新数据覆盖老数据。

（4）导入数据后，有可能数据还是存在不一致，那么就对数据进行校验，比对新老库的每条数据。如果存在差异，针对差异数据，执行（3）。循环（3）、（4）步骤，直至数据完全一致。

（5）修改应用程序配置，将数据只写入中间件。

（6）中间件根据分片规则，将数据分发到分库（分表）中。

### 主从升级

生产环境的数据库，为了保证高可用，一般会采用主备架构。主库支持读写操作，从库支持读操作。

![](http://dunwu.test.upcdn.net/snap/20200601121215.png)

由于主备节点数据一致，所以将从库升级为主节点，并修改分片配置，将从节点作为分库之一，就实现了扩容。

![](http://dunwu.test.upcdn.net/snap/20200601121400.png)

#### 升级从库的流程

（1）解除主从关系，从库升级为主库。

（2）应用程序，修改配置，读写通过中间件。

（3）分库分表中间，修改分片配置。将数据按照新的规则分发。

（4）编写临时程序，清理冗余数据。比如：原来是一个单库，数据量为 400 万。从节点升级为分库之一后，每个分库都有 400 万数据，其中 200 万是冗余数据。清理完后，进行数据校验。

（5）为每个分库添加新的从库，保证高可用。

#### 升级从库方案分析

优点：不需要停机，无需数据迁移。

缺点：

## 四、分库分表的问题

### 分布式 ID 问题

一旦数据库被切分到多个物理结点上，我们将不能再依赖数据库自身的主键生成机制。一方面，某个分区数据库自生成的 ID 无法保证在全局上是唯一的；另一方面，应用程序在插入数据之前需要先获得 ID,以便进行 SQL 路由. 一些常见的主键生成策略

#### UUID

使用 UUID 作主键是最简单的方案，但是缺点也是非常明显的。由于 UUID 非常的长，除占用大量存储空间外，最主要的问题是在索引上，在建立索引和基于索引进行查询时都存在性能问题。

#### 结合数据库维护一个 Sequence 表

此方案的思路也很简单，在数据库中建立一个 Sequence 表，表的结构类似于：

```SQL
CREATE TABLE `SEQUENCE` (
    `table_name` varchar(18) NOT NULL,
    `nextid` bigint(20) NOT NULL,
    PRIMARY KEY (`table_name`)
) ENGINE=InnoDB
```

每当需要为某个表的新纪录生成 ID 时就从 Sequence 表中取出对应表的 nextid,并将 nextid 的值加 1 后更新到数据库中以备下次使用。此方案也较简单，但缺点同样明显：由于所有插入任何都需要访问该表，该表很容易成为系统性能瓶颈，同时它也存在单点问题，一旦该表数据库失效，整个应用程序将无法工作。有人提出使用 Master-Slave 进行主从同步，但这也只能解决单点问题，并不能解决读写比为 1:1 的访问压力问题。

#### [Twitter 的分布式自增 ID 算法 Snowflake](http://blog.sina.com.cn/s/blog_6b7c2e660102vbi2.html)

在分布式系统中，需要生成全局 UID 的场合还是比较多的，twitter 的 snowflake 解决了这种需求，实现也还是很简单的，除去配置信息，核心代码就是毫秒级时间 41 位 机器 ID 10 位 毫秒内序列 12 位。

- 10---0000000000 0000000000 0000000000 0000000000 0 --- 00000 ---00000 ---000000000000 在上面的字符串中，第一位为未使用（实际上也可作为 long 的符号位），接下来的 41 位为毫秒级时间，然后 5 位 datacenter 标识位，5 位机器 ID（并不算标识符，实际是为线程标识），然后 12 位该毫秒内的当前毫秒内的计数，加起来刚好 64 位，为一个 Long 型。

这样的好处是，整体上按照时间自增排序，并且整个分布式系统内不会产生 ID 碰撞（由 datacenter 和机器 ID 作区分），并且效率较高，经测试，snowflake 每秒能够产生 26 万 ID 左右，完全满足需要。

### 分布式事务问题

目前有两种可行的方案：

- 方案一：使用分布式事务
  - 优点：交由数据库管理，简单有效
  - 缺点：性能代价高，特别是 sharding 越来越多时
- 方案二：由应用程序和数据库共同控制
  - 原理：将一个跨多个数据库的分布式事务分拆成多个仅处于单个数据库上面的小事务，并通过应用程序来总控各个小事务。
  - 优点：性能上有优势
  - 缺点：需要应用程序在事务控制上做灵活设计。如果使用 了[spring](http://lib.csdn.net/base/javaee)的事务管理，改动起来会面临一定的困难。

跨库事务也是分布式的数据库集群要面对的棘手事情。 合理采用分表，可以在降低单表数据量的情况下，尽量使用本地事务，善于使用同库不同表可有效避免分布式事务带来的麻烦。在不能避免跨库事务的场景，有些业务仍然需要保持事务的一致性。 而基于 XA 的分布式事务由于在并发度高的场景中性能无法满足需要，并未被互联网巨头大规模使用，他们大多采用最终一致性的柔性事务代替强一致事务。

### 跨节点 Join 和聚合

分库分表后，无法直接跨节点 `join` 、`count`、`order by`、`group by` 以及聚合。

针对这类问题，普遍做法是**二次查询**。

在第一次查询时，获取各个节点上的结果。

在程序中将这些结果进行合并、筛选。

### 跨分片的排序分页

一般来讲，分页时需要按照指定字段进行排序。当排序字段就是分片字段的时候，我们通过分片规则可以比较容易定位到指定的分片，而当排序字段非分片字段的时候，情况就会变得比较复杂了。为了最终结果的准确性，我们需要在不同的分片节点中将数据进行排序并返回，并将不同分片返回的结果集进行汇总和再次排序，最后再返回给用户。如下图所示：

![img](https://upload-images.jianshu.io/upload_images/3710706-925381b9a478c8df.png?imageMogr2/auto-orient/strip|imageView2/2/w/640/format/webp)

上面图中所描述的只是最简单的一种情况（取第一页数据），看起来对性能的影响并不大。但是，如果想取出第 10 页数据，情况又将变得复杂很多，如下图所示：

![img](https://upload-images.jianshu.io/upload_images/3710706-9a7cfbdb95bb9b70.png?imageMogr2/auto-orient/strip|imageView2/2/w/640/format/webp)

有些读者可能并不太理解，为什么不能像获取第一页数据那样简单处理（排序取出前 10 条再合并、排序）。其实并不难理解，因为各分片节点中的数据可能是随机的，为了排序的准确性，必须把所有分片节点的前 N 页数据都排序好后做合并，最后再进行整体的排序。很显然，这样的操作是比较消耗资源的，用户越往后翻页，系统性能将会越差。

那如何解决分库情况下的分页问题呢？有以下几种办法：

如果是在前台应用提供分页，则限定用户只能看前面 n 页，这个限制在业务上也是合理的，一般看后面的分页意义不大（如果一定要看，可以要求用户缩小范围重新查询）。

如果是后台批处理任务要求分批获取数据，则可以加大 page size，比如每次获取 5000 条记录，有效减少分页数（当然离线访问一般走备库，避免冲击主库）。

分库设计时，一般还有配套大数据平台汇总所有分库的记录，有些分页查询可以考虑走大数据平台。

## 五、中间件

国内常见分库分表中间件：

- [Cobar](https://github.com/alibaba/cobar) - 阿里 b2b 团队开发和开源的，属于 proxy 层方案，就是介于应用服务器和数据库服务器之间。应用程序通过 JDBC 驱动访问 cobar 集群，cobar 根据 SQL 和分库规则对 SQL 做分解，然后分发到 MySQL 集群不同的数据库实例上执行。早些年还可以用，但是最近几年都没更新了，基本没啥人用，差不多算是被抛弃的状态吧。而且不支持读写分离、存储过程、跨库 join 和分页等操作。
- [TDDL](https://github.com/alibaba/tb_tddl) - 淘宝团队开发的，属于 client 层方案。支持基本的 crud 语法和读写分离，但不支持 join、多表查询等语法。目前使用的也不多，因为还依赖淘宝的 diamond 配置管理系统。
- [Atlas](https://github.com/Qihoo360/Atlas) - 360 开源的，属于 proxy 层方案，以前是有一些公司在用的，但是确实有一个很大的问题就是社区最新的维护都在 5 年前了。所以，现在用的公司基本也很少了。
- [sharding-jdbc](https://github.com/dangdangdotcom/sharding-jdbc) - 当当开源的，属于 client 层方案。确实之前用的还比较多一些，因为 SQL 语法支持也比较多，没有太多限制，而且目前推出到了 2.0 版本，支持分库分表、读写分离、分布式 id 生成、柔性事务（最大努力送达型事务、TCC 事务）。而且确实之前使用的公司会比较多一些（这个在官网有登记使用的公司，可以看到从 2017 年一直到现在，是有不少公司在用的），目前社区也还一直在开发和维护，还算是比较活跃，个人认为算是一个现在也**可以选择的方案**。
- [Mycat](http://www.mycat.org.cn/) - 基于 cobar 改造的，属于 proxy 层方案，支持的功能非常完善，而且目前应该是非常火的而且不断流行的数据库中间件，社区很活跃，也有一些公司开始在用了。但是确实相比于 sharding jdbc 来说，年轻一些，经历的锤炼少一些。

技术选型建议：

建议使用的是 sharding-jdbc 和 mycat。

- [sharding-jdbc](https://github.com/dangdangdotcom/sharding-jdbc) 这种 client 层方案的**优点在于不用部署，运维成本低，不需要代理层的二次转发请求，性能很高**，但是如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要**耦合** sharding-jdbc 的依赖。其本质上通过配置多数据源，然后根据设定的分库分表策略，计算路由，将请求发送到计算得到的节点上。
- [Mycat](http://www.mycat.org.cn/) 这种 proxy 层方案的**缺点在于需要部署**，自己运维一套中间件，运维成本高，但是**好处在于对于各个项目是透明的**，如果遇到升级之类的都是自己中间件那里搞就行了。

通常来说，这两个方案其实都可以选用，但是我个人建议中小型公司选用 sharding-jdbc，client 层方案轻便，而且维护成本低，不需要额外增派人手，而且中小型公司系统复杂度会低一些，项目也没那么多；但是中大型公司最好还是选用 mycat 这类 proxy 层方案，因为可能大公司系统和项目非常多，团队很大，人员充足，那么最好是专门弄个人来研究和维护 mycat，然后大量项目直接透明使用即可。

## 参考资料

- [ShardingSphere 官方文档](https://shardingsphere.apache.org/document/current/cn/overview/)
- [“分库分表" ？选型和流程要慎重，否则会失控](https://juejin.im/post/5bf778ef5188251b8a26ed8b)
- [分库分表需要考虑的问题及方案](https://www.jianshu.com/p/32b3e91aa22c)
- [一次难得的分库分表实践](https://juejin.im/post/5d4b6dc1f265da03c1288332)
- [分库分表平滑扩容](https://www.cnblogs.com/barrywxx/p/11532122.html)
